# lamo

## Inspiration

* https://github.com/karpathy/nanoGPT/tree/master
* https://theaisummer.com/einsum-attention/
* https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html


## Versions

torch: 2.2.1+cu121
flash_attn: 2.5.6


## Environment

```
conda create --name malamo python=3.11
conda activate malamo
pip install torch
pip install flash-attn --no-build-isolation
```
